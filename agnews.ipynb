{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1M5_LcAjOLm836tktiW8NluWIlHNzkiDG","authorship_tag":"ABX9TyPQl1JiQkLYpIBDEsB3iG/v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xCUqu0bopOf","executionInfo":{"status":"ok","timestamp":1663139404605,"user_tz":-330,"elapsed":14245,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"960f134a-2a29-4a38-9d55-30c2edb1fb9d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["main_dir = '/content/drive/MyDrive/CSC4046 - Individual Research/text_classification'"],"metadata":{"id":"AXNQ5mPzohMo","executionInfo":{"status":"ok","timestamp":1663139406613,"user_tz":-330,"elapsed":5,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip3 install -r \"$main_dir/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf9AT_VVowZ3","executionInfo":{"status":"ok","timestamp":1663139418687,"user_tz":-330,"elapsed":8953,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"6c7ebb0b-e5f2-403a-9b70-d33efe2b2df9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.3.1\n","  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 14.8 MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 68.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2022.6.2)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 78.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 79.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=cc27dcb3d76124fbc50e909a721496fa341930ca1a10c296ba0563335ad6ff1b\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc2 transformers-3.3.1\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","\n","from transformers import logging\n","logging.set_verbosity_warning()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3i4Gn0oaozjw","executionInfo":{"status":"ok","timestamp":1663139428907,"user_tz":-330,"elapsed":5731,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"68be6cfc-1fc3-4dee-f0dc-31dda7c8e844"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["!export CUDA_DEVICE_ORDER=PCI_BUS_ID\n","!export CUDA_VISIBLE_DEVICES=0,1"],"metadata":{"id":"QLFoEb4zo1ZY","executionInfo":{"status":"ok","timestamp":1663139433923,"user_tz":-330,"elapsed":22,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!ls '{main_dir}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2ADvA5BtB9J","executionInfo":{"status":"ok","timestamp":1663139438256,"user_tz":-330,"elapsed":954,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"1d6a49dc-5513-4592-b9a0-15430ecb0cf9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["agnews.ipynb  datasets\tREADME.md  requirements.txt  src\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Kfr-LCWRtRFi","executionInfo":{"status":"ok","timestamp":1663139701740,"user_tz":-330,"elapsed":561,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"outputs":[],"source":["DATASET='agnews'\n","LABEL_NAME_FILE='label_names.txt'\n","TRAIN_CORPUS='train.txt'\n","TEST_CORPUS='test.txt'\n","TEST_LABEL='test_labels.txt'\n","MAX_LEN=200\n","TRAIN_BATCH=64\n","ACCUM_STEP=2\n","EVAL_BATCH=128\n","GPUS=1\n","MCP_EPOCH=3\n","SELF_TRAIN_EPOCH=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOQdrlkpEZKD","executionInfo":{"status":"ok","timestamp":1662801189474,"user_tz":-330,"elapsed":4889249,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"c8e835c9-aa53-48e0-d816-022d54666edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=2, category_vocab_size=100, dataset_dir='/content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=64, train_file='train.txt', update_interval=50)\n","Effective training batch size: 128\n","Downloading: 100% 232k/232k [00:00<00:00, 420kB/s]\n","Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n","Downloading: 100% 433/433 [00:00<00:00, 409kB/s]\n","Downloading: 100% 440M/440M [00:09<00:00, 47.9MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.txt\n","Converting texts into tensors.\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/label_name_data.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test.txt\n","Converting texts into tensors.\n","tcmalloc: large alloc 1104715776 bytes == 0xb3e68000 @  0x7f137efce615 0x592b76 0x4df71e 0x593605 0x515244 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test.pt\n","Reading labels from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test_labels.txt\n","Contructing category vocabulary.\n","100% 62/62 [01:05<00:00,  1.06s/it]\n","Class 0 category vocabulary: ['politics', 'political', 'politicians', 'government', 'elections', 'politician', 'democracy', 'democratic', 'governing', 'party', 'state', 'leadership', 'election', 'politically', 'affairs', 'issues', 'governments', 'voters', 'debate', 'cabinet', 'congress', 'democrat', 'administration', 'president', 'religion', 'republican', 'history', 'crisis', 'war', 'legislature', 'candidates', 'governance', 'pr', 'opposition', 'problems', 'relations', 'finance', 'justice', 'struggle', 'rhetoric', 'right', 'convention', 'votes', 'fighting', 'violence', 'senate', 'matters', 'fight', 'us', 'parliament', 'republicans', 'trouble', 'one', 'conflict', 'soil', 'voting', 'law', 'parliamentary', 'representation', 'house', 'reality', 'wars', 'campaign', 'contest', 'candidate', 'campaigns', 'legislative', 'transition', 'question', 'choice']\n","\n","Class 1 category vocabulary: ['sports', 'games', 'sporting', 'athletics', 'game', 'national', 'news', 'athletic', 'espn', 'soccer', 'stadium', 'basketball', 'arts', 'racing', 'baseball', 'tv', 'hockey', 'pro', 'press', 'team', 'red', 'home', 'bay', 'kings', 'legends', 'city', 'winning', 'miracle', 'olympic', 'go', 'giants', 'champions', 'ball', 'players', 'boxing', 'prime', 'teams', 'athletes', 'tennis', 'club', 'blue', 'coaches', 'gold', 'west', 'toronto', 'classic', 'pittsburgh', 'super', 'nfl', 'magic', 'key', 'times', 'field', 'warriors', 'rogers', 'stars', 'gym', 'championship', 'losses', 'college', 'mlb', 'veterans', 'rugby', 'hits', 'sun', 'bc', 'events', 'south', 'nba']\n","\n","Class 2 category vocabulary: ['business', 'businesses', 'trade', 'commercial', 'enterprise', 'shop', 'money', 'market', 'commerce', 'corporate', 'global', 'future', 'sales', 'general', 'group', 'retail', 'companies', 'management', 'operations', 'operation', 'corporation', 'store', 'division', 'firm', 'venture', 'brand', 'contract', 'revenue', 'economic', 'branch', 'subsidiary', 'personal', 'cash', 'short', 'line', 'bank', 'customer', 'concern', 'growth', 'chain', 'strategic', 'family', 'work', 'products', 'big', 'scientific', 'virtual', 'engineering', 'sector', 'trading', 'portfolio', 'ceo', 'segment', 'investment', 'working', 'executive', 'private', 'services', 'public', 'job', 'marketing']\n","\n","Class 3 category vocabulary: ['technology', 'technologies', 'tech', 'software', 'technological', 'device', 'equipment', 'hardware', 'infrastructure', 'devices', 'system', 'knowledge', 'technique', 'digital', 'technical', 'concept', 'systems', 'gear', 'techniques', 'material', 'functionality', 'process', 'facility', 'feature', 'capability', 'content', 'method', 'security', 'ability', 'network', 'internet', 'computing', 'chip', 'smart', 'modern', 'communication', 'language', 'mechanism', 'computer', 'design', 'cyber', 'standard', 'tool', 'development', 'format', 'protocol', 'wireless', 'phone', 'information', 'program', 'ce', 'plant', 'large', 'data', 'project', 'application', 'theory', 'science', 'performance', 'common', 'os', 'ict', 'speed', 'sensor', 'capabilities', 'electronic', 'society', 'silicon', 'invention', 'memory']\n","\n","Preparing self supervision for masked category prediction.\n","100% 938/938 [14:34<00:00,  1.07it/s]\n","Number of documents with category indicative terms found for each category is: {0: 211, 1: 1365, 2: 1426, 3: 2049}\n","There are totally 5051 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","  0% 0/79 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","100% 79/79 [01:28<00:00,  1.11s/it]\n","Average training loss: 0.8295076489448547\n","Epoch 2:\n","100% 79/79 [01:27<00:00,  1.11s/it]\n","Average training loss: 0.2600099742412567\n","Epoch 3:\n","100% 79/79 [01:27<00:00,  1.11s/it]\n","Average training loss: 0.13571977615356445\n","\n","Start self-training.\n","  0% 0/100 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 5.336e-07\n","Average training loss: 0.12719884514808655\n","Test acc: 0.8255262970924377\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 9.925e-07\n","Average training loss: 0.09110914170742035\n","Test acc: 0.8392105102539062\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 9.332e-07\n","Average training loss: 0.08773031830787659\n","Test acc: 0.8475000262260437\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 8.739e-07\n","Average training loss: 0.08103155344724655\n","Test acc: 0.8522368669509888\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 8.147e-07\n","Average training loss: 0.06951514631509781\n","Test acc: 0.8552631735801697\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 7.554e-07\n","Average training loss: 0.06953871995210648\n","Test acc: 0.8573684096336365\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 6.961e-07\n","Average training loss: 0.06643828004598618\n","Test acc: 0.8582894802093506\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 6.368e-07\n","Average training loss: 0.062469832599163055\n","Test acc: 0.8596052527427673\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 5.775e-07\n","Average training loss: 0.05833034589886665\n","Test acc: 0.8614473938941956\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 5.182e-07\n","Average training loss: 0.055957235395908356\n","Test acc: 0.8627631664276123\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 4.589e-07\n","Average training loss: 0.05636807531118393\n","Test acc: 0.862500011920929\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 3.996e-07\n","Average training loss: 0.05316061154007912\n","Test acc: 0.8626315593719482\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 3.403e-07\n","Average training loss: 0.054087426513433456\n","Test acc: 0.8626315593719482\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 2.81e-07\n","Average training loss: 0.05107941851019859\n","Test acc: 0.8622368574142456\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 2.217e-07\n","Average training loss: 0.05365229398012161\n","Test acc: 0.8636842370033264\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 1.625e-07\n","Average training loss: 0.05374560132622719\n","Test acc: 0.8636842370033264\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 1.032e-07\n","Average training loss: 0.05125279352068901\n","Test acc: 0.8638157844543457\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 4.388e-08\n","Average training loss: 0.04980086535215378\n","Test acc: 0.8638157844543457\n","Saving final model to /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/final_model.pt\n","\n","Loading final model from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/final_model.pt\n","Writing prediction results to /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/out.txt\n"]}],"source":["# without augmentation\n","! python '{main_dir}/src/train.py' --dataset_dir '{main_dir}/datasets/{DATASET}' --label_names_file '{LABEL_NAME_FILE}' --train_file '{TRAIN_CORPUS}' --test_file '{TEST_CORPUS}' --test_label_file '{TEST_LABEL}' --max_len $MAX_LEN --train_batch_size $TRAIN_BATCH --accum_steps $ACCUM_STEP --eval_batch_size $EVAL_BATCH --gpus $GPUS --mcp_epochs $MCP_EPOCH --self_train_epochs $SELF_TRAIN_EPOCH"]},{"cell_type":"code","source":["# after augmentation\n","! python '{main_dir}/src/train.py' --dataset_dir '{main_dir}/datasets/{DATASET}/augmented_data' --label_names_file '{LABEL_NAME_FILE}' --train_file '{TRAIN_CORPUS}' --test_file '{TEST_CORPUS}' --test_label_file '{TEST_LABEL}' --max_len $MAX_LEN --train_batch_size $TRAIN_BATCH --accum_steps $ACCUM_STEP --eval_batch_size $EVAL_BATCH --gpus $GPUS --mcp_epochs $MCP_EPOCH --self_train_epochs $SELF_TRAIN_EPOCH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yFN0QR51v-D","executionInfo":{"status":"ok","timestamp":1663147650435,"user_tz":-330,"elapsed":7938584,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"9a2c1a42-ec85-486e-a2bc-5ec2e9b1581a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=2, category_vocab_size=100, dataset_dir='/content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=64, train_file='train.txt', update_interval=50)\n","Effective training batch size: 128\n","Downloading: 100% 232k/232k [00:00<00:00, 893kB/s] \n","Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n","Downloading: 100% 433/433 [00:00<00:00, 467kB/s]\n","Downloading: 100% 440M/440M [00:05<00:00, 78.2MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/train.txt\n","Converting texts into tensors.\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/train.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/label_name_data.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/test.txt\n","Converting texts into tensors.\n","tcmalloc: large alloc 1111416832 bytes == 0x187d28000 @  0x7f97a1ce32a4 0x592b76 0x4df71e 0x593605 0x515244 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8\n","tcmalloc: large alloc 1389273088 bytes == 0x7f962b356000 @  0x7f97a1ce3615 0x592b76 0x4df71e 0x593605 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7\n","tcmalloc: large alloc 1736597504 bytes == 0x7f95c3b30000 @  0x7f97a1ce3615 0x592b76 0x4df71e 0x593605 0x515244 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8\n","tcmalloc: large alloc 2170748928 bytes == 0x7f9542500000 @  0x7f97a1ce3615 0x592b76 0x4df71e 0x593605 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7\n","tcmalloc: large alloc 1805033472 bytes == 0x7adf4000 @  0x7fa07be831e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804967936 bytes == 0x7adf4000 @  0x7fa07be831e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804902400 bytes == 0x7adf4000 @  0x7fa07be831e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804836864 bytes == 0x7adf4000 @  0x7fa07be831e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804771328 bytes == 0x7adf4000 @  0x7fa07be831e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 2018787328 bytes == 0x7adf4000 @  0x7fa07be84615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244 0x549576\n","tcmalloc: large alloc 1736597504 bytes == 0x7f95951ea000 @  0x7f97a1ce3615 0x592b76 0x4df71e 0x593605 0x515244 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8\n","tcmalloc: large alloc 2170748928 bytes == 0x7f95fca10000 @  0x7f97a1ce3615 0x592b76 0x4df71e 0x593605 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7\n","tcmalloc: large alloc 1805025280 bytes == 0x7bba6000 @  0x7fbbbfcb21e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804959744 bytes == 0x7bba6000 @  0x7fbbbfcb21e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804894208 bytes == 0x7bba6000 @  0x7fbbbfcb21e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804828672 bytes == 0x7bba6000 @  0x7fbbbfcb21e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804763136 bytes == 0x7bba6000 @  0x7fbbbfcb21e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 2153373696 bytes == 0x14880c000 @  0x7fbbbfcb3615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244 0x549576\n","tcmalloc: large alloc 1805017088 bytes == 0x7a266000 @  0x7f8662c591e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804959744 bytes == 0x7a266000 @  0x7f8662c591e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804894208 bytes == 0x7a266000 @  0x7f8662c591e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804828672 bytes == 0x7a266000 @  0x7f8662c591e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 1804763136 bytes == 0x7a266000 @  0x7f8662c591e7 0x4a3940 0x5b438c 0x5c9ec7 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244\n","tcmalloc: large alloc 2018787328 bytes == 0x7a266000 @  0x7f8662c5a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173 0x62a809 0x59358d 0x515244 0x549576\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/test.pt\n","Reading labels from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/augmented_data/test_labels.txt\n","Contructing category vocabulary.\n","100% 191/191 [06:21<00:00,  2.00s/it]\n","Class 0 category vocabulary: ['politics', 'political', 'government', 'politicians', 'elections', 'politician', 'democracy', 'affairs', 'party', 'issues', 'democratic', 'election', 'politically', 'cabinet', 'state', 'governing', 'leadership', 'justice', 'governments', 'administration', 'congress', 'war', 'president', 'finance', 'opposition', 'voters', 'religion', 'candidates', 'democrat', 'crisis', 'problems', 'debate', 'legislature', 'motion', 'parliament', 'leaders', 'us', 'governance', 'matters', 'wars', 'pr', 'senate', 'people', 'labour', 'republican', 'law', 'house', 'relations', 'question', 'votes', 'rules', 'rhetoric', 'parties', 'violence', 'campaign', 'struggle', 'usa', 'questions', 'forces', 'voting', 'fight', 'conflict', 'democrats', 'america', 'parliamentary', 'things', 'right', 'youth']\n","\n","Class 1 category vocabulary: ['sports', 'games', 'sporting', 'game', 'athletics', 'news', 'national', 'athletic', 'football', 'soccer', 'espn', 'stadium', 'baseball', 'basketball', 'pro', 'racing', 'hockey', 'tv', 'champions', 'times', 'go', 'boxing', 'press', 'events', 'red', 'team', 'legends', 'tennis', 'olympic', 'coaches', 'super', 'arts', 'radio', 'athletes', 'club', 'win', 'sun', 'former', 'home', 'blue', 'championship', 'field', 'prime', 'rugby', 'classic', 'talk', 'winning', 'ball', 'gold', 'nfl', 'bay', 'finals', 'players', 'play', 'gym', 'ring', 'matches', 'stars', 'cycling', 'next', 'cup', 'cable']\n","\n","Class 2 category vocabulary: ['business', 'businesses', 'trade', 'enterprise', 'commercial', 'shop', 'market', 'money', 'commerce', 'group', 'corporate', 'global', 'general', 'sales', 'international', 'management', 'short', 'operations', 'retail', 'companies', 'operation', 'career', 'store', 'personal', 'venture', 'corporation', 'family', 'division', 'firm', 'brand', 'economic', 'property', 'subsidiary', 'big', 'cash', 'work', 'bank', 'branch', 'concern', 'growth', 'revenue', 'marketing', 'industrial', 'contract', 'strategic', 'private', 'trading', 'products', 'services', 'activities', 'middle', 'virtual', 'public', 'fast', 'customer', 'job', 'name', 'military', 'account', 'line']\n","\n","Class 3 category vocabulary: ['technology', 'technologies', 'tech', 'software', 'device', 'technological', 'equipment', 'system', 'infrastructure', 'devices', 'hardware', 'knowledge', 'content', 'technique', 'concept', 'ability', 'communication', 'digital', 'technical', 'systems', 'internet', 'techniques', 'society', 'functionality', 'capability', 'information', 'method', 'gear', 'modern', 'feature', 'material', 'format', 'process', 'facility', 'language', 'theory', 'network', 'smart', 'application', 'time', 'ict', 'plant', 'tool', 'design', 'security', 'project', 'computing', 'data', 'science', 'privacy', 'chip', 'computer', 'standard', 'ce', 'large', 'cyber', 'phone', 'invention', 'mechanism', 'industries', 'social', 'edge', 'tools', 'performance', 'speed']\n","\n","Preparing self supervision for masked category prediction.\n","100% 2813/2813 [1:35:04<00:00,  2.03s/it]\n","Number of documents with category indicative terms found for each category is: {0: 471, 1: 2725, 2: 3453, 3: 5159}\n","There are totally 11808 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","  0% 0/185 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","100% 185/185 [06:06<00:00,  1.98s/it]\n","Average training loss: 0.6155188083648682\n","Epoch 2:\n","100% 185/185 [06:05<00:00,  1.98s/it]\n","Average training loss: 0.1165751963853836\n","Epoch 3:\n","100% 185/185 [06:05<00:00,  1.97s/it]\n","Average training loss: 0.053671661764383316\n","\n","Start self-training.\n","  0% 0/100 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","  2% 2/100 [00:04<03:47,  2.32s/it]\n","CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 14.76 GiB total capacity; 12.06 GiB already allocated; 94.75 MiB free; 12.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","Your GPUs can't hold the current batch size for training, try to reduce `--train_batch_size`, current: 64\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/CSC4046 - Individual Research/text_classification/src/train.py\", line 66, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/CSC4046 - Individual Research/text_classification/src/train.py\", line 59, in main\n","    trainer.self_train(epochs=args.self_train_epochs, loader_name=args.final_model)\n","  File \"/content/drive/MyDrive/Level 4/CSC4046 - Individual Research/text_classification/src/trainer.py\", line 566, in self_train\n","    mp.spawn(self.self_train_dist, nprocs=self.world_size, args=(epochs, loader_name))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\n","    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\n","    while not context.join():\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 154, in join\n","    exit_code=exitcode\n","torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1\n"]}]}]}