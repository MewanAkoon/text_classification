{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1M5_LcAjOLm836tktiW8NluWIlHNzkiDG","authorship_tag":"ABX9TyMbT2CZl1zpC6w/j5Jdt4u/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xCUqu0bopOf","executionInfo":{"status":"ok","timestamp":1662796213526,"user_tz":-330,"elapsed":29997,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"697816d2-3c50-49a2-8bfc-18df4b3886e1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["main_dir = '/content/drive/MyDrive/CSC4046 - Individual Research/text_classification'"],"metadata":{"id":"AXNQ5mPzohMo","executionInfo":{"status":"ok","timestamp":1662796220198,"user_tz":-330,"elapsed":591,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip3 install -r \"$main_dir/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf9AT_VVowZ3","executionInfo":{"status":"ok","timestamp":1662796237141,"user_tz":-330,"elapsed":11407,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"6c80437d-34af-46fb-e518-75f521e80ab4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.3.1\n","  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 67.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (4.64.0)\n","Collecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 65.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2022.6.2)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 59.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (2022.6.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/requirements.txt (line 1)) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1514b5c26bae4cafc36e7a8cc65dae46c980bb2cb1726b4a22f1a0f319a447dd\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc2 transformers-3.3.1\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","\n","from transformers import logging\n","logging.set_verbosity_warning()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3i4Gn0oaozjw","executionInfo":{"status":"ok","timestamp":1662796256086,"user_tz":-330,"elapsed":6561,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"2a5c6807-2910-43b6-9c89-2d60d6a17af1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["!export CUDA_DEVICE_ORDER=PCI_BUS_ID\n","!export CUDA_VISIBLE_DEVICES=0,1"],"metadata":{"id":"QLFoEb4zo1ZY","executionInfo":{"status":"ok","timestamp":1662796271825,"user_tz":-330,"elapsed":722,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!ls '{main_dir}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2ADvA5BtB9J","executionInfo":{"status":"ok","timestamp":1662796277975,"user_tz":-330,"elapsed":12,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"f19e9744-b655-4ded-e12d-6be74582f4ea"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["agnews.ipynb  datasets\t     imdb.ipynb  requirements.txt\n","amazon.ipynb  dbpedia.ipynb  README.md\t src\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Kfr-LCWRtRFi","executionInfo":{"status":"ok","timestamp":1662796288455,"user_tz":-330,"elapsed":596,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}}},"outputs":[],"source":["DATASET='agnews'\n","LABEL_NAME_FILE='label_names.txt'\n","TRAIN_CORPUS='train.txt'\n","TEST_CORPUS='test.txt'\n","TEST_LABEL='test_labels.txt'\n","MAX_LEN=200\n","TRAIN_BATCH=64\n","ACCUM_STEP=2\n","EVAL_BATCH=128\n","GPUS=1\n","MCP_EPOCH=3\n","SELF_TRAIN_EPOCH=1"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOQdrlkpEZKD","executionInfo":{"status":"ok","timestamp":1662801189474,"user_tz":-330,"elapsed":4889249,"user":{"displayName":"Damitha Amarakoon","userId":"09631627231718956732"}},"outputId":"c8e835c9-aa53-48e0-d816-022d54666edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=2, category_vocab_size=100, dataset_dir='/content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=64, train_file='train.txt', update_interval=50)\n","Effective training batch size: 128\n","Downloading: 100% 232k/232k [00:00<00:00, 420kB/s]\n","Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n","Downloading: 100% 433/433 [00:00<00:00, 409kB/s]\n","Downloading: 100% 440M/440M [00:09<00:00, 47.9MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.txt\n","Converting texts into tensors.\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/label_name_data.pt\n","Reading texts from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test.txt\n","Converting texts into tensors.\n","tcmalloc: large alloc 1104715776 bytes == 0xb3e68000 @  0x7f137efce615 0x592b76 0x4df71e 0x593605 0x515244 0x593dd7 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8\n","Saving encoded texts into /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test.pt\n","Reading labels from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/test_labels.txt\n","Contructing category vocabulary.\n","100% 62/62 [01:05<00:00,  1.06s/it]\n","Class 0 category vocabulary: ['politics', 'political', 'politicians', 'government', 'elections', 'politician', 'democracy', 'democratic', 'governing', 'party', 'state', 'leadership', 'election', 'politically', 'affairs', 'issues', 'governments', 'voters', 'debate', 'cabinet', 'congress', 'democrat', 'administration', 'president', 'religion', 'republican', 'history', 'crisis', 'war', 'legislature', 'candidates', 'governance', 'pr', 'opposition', 'problems', 'relations', 'finance', 'justice', 'struggle', 'rhetoric', 'right', 'convention', 'votes', 'fighting', 'violence', 'senate', 'matters', 'fight', 'us', 'parliament', 'republicans', 'trouble', 'one', 'conflict', 'soil', 'voting', 'law', 'parliamentary', 'representation', 'house', 'reality', 'wars', 'campaign', 'contest', 'candidate', 'campaigns', 'legislative', 'transition', 'question', 'choice']\n","\n","Class 1 category vocabulary: ['sports', 'games', 'sporting', 'athletics', 'game', 'national', 'news', 'athletic', 'espn', 'soccer', 'stadium', 'basketball', 'arts', 'racing', 'baseball', 'tv', 'hockey', 'pro', 'press', 'team', 'red', 'home', 'bay', 'kings', 'legends', 'city', 'winning', 'miracle', 'olympic', 'go', 'giants', 'champions', 'ball', 'players', 'boxing', 'prime', 'teams', 'athletes', 'tennis', 'club', 'blue', 'coaches', 'gold', 'west', 'toronto', 'classic', 'pittsburgh', 'super', 'nfl', 'magic', 'key', 'times', 'field', 'warriors', 'rogers', 'stars', 'gym', 'championship', 'losses', 'college', 'mlb', 'veterans', 'rugby', 'hits', 'sun', 'bc', 'events', 'south', 'nba']\n","\n","Class 2 category vocabulary: ['business', 'businesses', 'trade', 'commercial', 'enterprise', 'shop', 'money', 'market', 'commerce', 'corporate', 'global', 'future', 'sales', 'general', 'group', 'retail', 'companies', 'management', 'operations', 'operation', 'corporation', 'store', 'division', 'firm', 'venture', 'brand', 'contract', 'revenue', 'economic', 'branch', 'subsidiary', 'personal', 'cash', 'short', 'line', 'bank', 'customer', 'concern', 'growth', 'chain', 'strategic', 'family', 'work', 'products', 'big', 'scientific', 'virtual', 'engineering', 'sector', 'trading', 'portfolio', 'ceo', 'segment', 'investment', 'working', 'executive', 'private', 'services', 'public', 'job', 'marketing']\n","\n","Class 3 category vocabulary: ['technology', 'technologies', 'tech', 'software', 'technological', 'device', 'equipment', 'hardware', 'infrastructure', 'devices', 'system', 'knowledge', 'technique', 'digital', 'technical', 'concept', 'systems', 'gear', 'techniques', 'material', 'functionality', 'process', 'facility', 'feature', 'capability', 'content', 'method', 'security', 'ability', 'network', 'internet', 'computing', 'chip', 'smart', 'modern', 'communication', 'language', 'mechanism', 'computer', 'design', 'cyber', 'standard', 'tool', 'development', 'format', 'protocol', 'wireless', 'phone', 'information', 'program', 'ce', 'plant', 'large', 'data', 'project', 'application', 'theory', 'science', 'performance', 'common', 'os', 'ict', 'speed', 'sensor', 'capabilities', 'electronic', 'society', 'silicon', 'invention', 'memory']\n","\n","Preparing self supervision for masked category prediction.\n","100% 938/938 [14:34<00:00,  1.07it/s]\n","Number of documents with category indicative terms found for each category is: {0: 211, 1: 1365, 2: 1426, 3: 2049}\n","There are totally 5051 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","  0% 0/79 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","100% 79/79 [01:28<00:00,  1.11s/it]\n","Average training loss: 0.8295076489448547\n","Epoch 2:\n","100% 79/79 [01:27<00:00,  1.11s/it]\n","Average training loss: 0.2600099742412567\n","Epoch 3:\n","100% 79/79 [01:27<00:00,  1.11s/it]\n","Average training loss: 0.13571977615356445\n","\n","Start self-training.\n","  0% 0/100 [00:00<?, ?it/s][W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 5.336e-07\n","Average training loss: 0.12719884514808655\n","Test acc: 0.8255262970924377\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 9.925e-07\n","Average training loss: 0.09110914170742035\n","Test acc: 0.8392105102539062\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 9.332e-07\n","Average training loss: 0.08773031830787659\n","Test acc: 0.8475000262260437\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 8.739e-07\n","Average training loss: 0.08103155344724655\n","Test acc: 0.8522368669509888\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 8.147e-07\n","Average training loss: 0.06951514631509781\n","Test acc: 0.8552631735801697\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 7.554e-07\n","Average training loss: 0.06953871995210648\n","Test acc: 0.8573684096336365\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 6.961e-07\n","Average training loss: 0.06643828004598618\n","Test acc: 0.8582894802093506\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 6.368e-07\n","Average training loss: 0.062469832599163055\n","Test acc: 0.8596052527427673\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 5.775e-07\n","Average training loss: 0.05833034589886665\n","Test acc: 0.8614473938941956\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 5.182e-07\n","Average training loss: 0.055957235395908356\n","Test acc: 0.8627631664276123\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 4.589e-07\n","Average training loss: 0.05636807531118393\n","Test acc: 0.862500011920929\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 3.996e-07\n","Average training loss: 0.05316061154007912\n","Test acc: 0.8626315593719482\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 3.403e-07\n","Average training loss: 0.054087426513433456\n","Test acc: 0.8626315593719482\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 2.81e-07\n","Average training loss: 0.05107941851019859\n","Test acc: 0.8622368574142456\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 2.217e-07\n","Average training loss: 0.05365229398012161\n","Test acc: 0.8636842370033264\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 1.625e-07\n","Average training loss: 0.05374560132622719\n","Test acc: 0.8636842370033264\n","100% 100/100 [01:51<00:00,  1.11s/it]\n","lr: 1.032e-07\n","Average training loss: 0.05125279352068901\n","Test acc: 0.8638157844543457\n","100% 100/100 [01:50<00:00,  1.11s/it]\n","lr: 4.388e-08\n","Average training loss: 0.04980086535215378\n","Test acc: 0.8638157844543457\n","Saving final model to /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/final_model.pt\n","\n","Loading final model from /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/final_model.pt\n","Writing prediction results to /content/drive/MyDrive/CSC4046 - Individual Research/text_classification/datasets/agnews/out.txt\n"]}],"source":["! python '{main_dir}/src/train.py' --dataset_dir '{main_dir}/datasets/{DATASET}' --label_names_file '{LABEL_NAME_FILE}' --train_file '{TRAIN_CORPUS}' --test_file '{TEST_CORPUS}' --test_label_file '{TEST_LABEL}' --max_len $MAX_LEN --train_batch_size $TRAIN_BATCH --accum_steps $ACCUM_STEP --eval_batch_size $EVAL_BATCH --gpus $GPUS --mcp_epochs $MCP_EPOCH --self_train_epochs $SELF_TRAIN_EPOCH"]}]}